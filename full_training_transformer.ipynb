{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os, glob\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['question', 'subject', 'choices', 'answer'],\n",
       "        num_rows: 14042\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'subject', 'choices', 'answer'],\n",
       "        num_rows: 1531\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['question', 'subject', 'choices', 'answer'],\n",
       "        num_rows: 285\n",
       "    })\n",
       "    auxiliary_train: Dataset({\n",
       "        features: ['question', 'subject', 'choices', 'answer'],\n",
       "        num_rows: 99842\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"cais/mmlu\", 'all')\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "model_path = \"databricks/dolly-v2-3b\"\n",
    "#model_path = \"ibm-granite/granite-3b-code-base\"\n",
    "#model_path = \"ibm-granite/granite-7b-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_prompt(data_point):\n",
    "    prompt = f\"\"\" Given a question and the possible answer choices, give the index of the right choice.\n",
    "    ### Question\n",
    "    {data_point['question']}\n",
    "    ### Choices\n",
    "    {data_point['choices']}\n",
    "    ### Answer\n",
    "    {data_point['answer']}\n",
    "\"\"\"\n",
    "    return tokenizer(prompt,\n",
    "                     padding=\"max_length\",\n",
    "                     max_length=256,\n",
    "                     truncation=True)\n",
    "\n",
    "# def tokenize(prompt):\n",
    "#     return tokenizer(prompt,\n",
    "#                      padding=\"max_length\",\n",
    "#                      truncation=True)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_datasets = dataset.map(tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = dataset['auxiliary_train'].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = dataset['validation'].shuffle(seed=42).select(range(200))\n",
    "\n",
    "small_train_dataset_tokenized = small_train_dataset.map(tokenize_prompt)\n",
    "small_eval_dataset_tokenized = small_eval_dataset.map(tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(output_dir=\"train_output\",\n",
    "                                   eval_strategy=\"steps\",\n",
    "                                   max_steps=1000,\n",
    "                                   eval_steps=50,\n",
    "                                   save_steps=100,\n",
    "                                   learning_rate=2.5e-5,\n",
    "                                   fp16=True,\n",
    "                                   per_device_train_batch_size=1, # This improved the memory utilization\n",
    "                                   gradient_accumulation_steps=4, # Combined with the above\n",
    "                                   gradient_checkpointing=True, # and this\n",
    "                                   optim=\"adamw_bnb_8bit\",\n",
    "                                   do_eval=True,\n",
    "                                   report_to=\"none\")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer,\n",
    "                                               mlm=False)\n",
    "# MLM false: masked lang model: false is for causal language model; labels are a copy of the input so the collator (GPT like)\n",
    "# MLM true: randomly mask tokens in a seq and the model predicts that (BERT like)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset_tokenized,\n",
    "    eval_dataset=small_eval_dataset_tokenized,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer._train_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-18 16:24:01,024] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m please install triton==1.0.0 if you want to use sparse attention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/shanand/ilab/lib64/python3.12/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/shanand/ilab/lib64/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with all the gradient checkpointing, accumulation, fp16 training, and changing the adam optimizer, the two GPUs still go OOM.\n",
    "\n",
    "Moving on to accelerate, bitsandbytes, and deepspeed\n",
    "\n",
    "Next would be LORA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we want to use the local copy of the mmlu dataset\n",
    "# We can create something like this\n",
    "# However this has to be debugged\n",
    "# IMO the local csvs should be created keeping datasets in mind\n",
    "\n",
    "MMLU_DATA_PATH = '/usr/data/mmlu'\n",
    "TRAIN_DATA = os.path.join(MMLU_DATA_PATH, 'auxiliary_train')\n",
    "TEST_DATA = os.path.join(MMLU_DATA_PATH, 'test')\n",
    "\n",
    "\n",
    "\n",
    "def combine_csv_files(directory, output_file=None):\n",
    "    \"\"\"\n",
    "    Combine all CSV files in the specified directory into a single DataFrame and add a column for the filename.\n",
    "\n",
    "    Parameters:\n",
    "    directory (str): The path to the directory containing the CSV files.\n",
    "    output_file (str, optional): The path to save the combined DataFrame as a CSV file. Default is None.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The combined DataFrame.\n",
    "    \"\"\"\n",
    "    # Get a list of all CSV files in the directory\n",
    "    csv_files = glob.glob(os.path.join(directory, '*.csv'))\n",
    "\n",
    "    # List to store individual DataFrames\n",
    "    dfs = []\n",
    "\n",
    "    # Read each CSV file into a DataFrame and add a column for the filename\n",
    "    for csv_file in csv_files:\n",
    "        # Extract the file name without the extension\n",
    "        file_name = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(csv_file)\n",
    "        # Add a column for the filename\n",
    "        df['filename'] = file_name\n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Optionally, save the combined DataFrame to a new CSV file\n",
    "    if output_file:\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "train_data = combine_csv_files(TEST_DATA)\n",
    "train_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
